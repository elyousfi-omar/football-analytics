{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omar El Yousfi\n",
    "# Graduation Project\n",
    "# Master Data Science and Intelligent Systems\n",
    "##  `Ploy-Disciplinary Faculty of Nador`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will present the backend code that we can't include in the application due to time complexity such as training a model and merging hundreds of dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib.patches import Arc\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from tkinter import filedialog\n",
    "import sys\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Creating shots dataset of a team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is merging event data to create shots dataset of football teams, we will use `Real Betis` as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an empty dataframe with the desired columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_df = pd.DataFrame(columns = [\"Team\", \"Player\", \"Body part\",\"Shot type\", \"Location\", \"Shot_Outcome\", \"statsbomb_xg\", \"Season\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path variable stores all the files in the events repository\n",
    "path = os.listdir('C:\\\\Users\\\\OMAR\\\\Desktop\\\\PFE\\\\Interface\\\\events\\\\')\n",
    "\n",
    "# Paths that have the files we would need\n",
    "events = 'C:\\\\Users\\\\OMAR\\\\Desktop\\\\PFE\\\\Interface\\\\events\\\\'\n",
    "matches = 'C:\\\\Users\\\\OMAR\\\\Desktop\\\\PFE\\\\Interface\\\\matches\\\\'\n",
    "\n",
    "# Matches repository has multiple repositories named after the id of the competition, we have to loop through each one\n",
    "# of them, the id's available are:\n",
    "competitions_id = [2,11,16,37,43,49,72]\n",
    "\n",
    "# Loop through each file in the events repository\n",
    "for file_name in path:\n",
    "    with open(events + file_name) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    df_t = pd.json_normalize(data, sep = \"_\").assign(match_id = file_name[:-5])\n",
    "    # Store the match's id\n",
    "    mid = df_t[\"match_id\"][0]\n",
    "    \n",
    "    # Keep the events that are shots and of Real Betis\n",
    "    shots = df_t.loc[df_t[\"type_name\"] == \"Shot\"].set_index(\"id\")\n",
    "    shots = shots.loc[shots[\"team_name\"] == \"Real Betis\"]\n",
    "    \n",
    "    # Loop through the matches repository:\n",
    "    for competi in competitions_id:\n",
    "        files = os.listdir(matches + str(competi) )\n",
    "        for file in files:\n",
    "            with open(matches+ str(competi) + \"/\" + file) as f:\n",
    "                temp = json.load(f)\n",
    "            # Each file has multiple matches, we have to look for those that have the same match id as the match desired\n",
    "            for i in range(len(temp)):\n",
    "                if temp[i][\"match_id\"] == pd.to_numeric(mid):\n",
    "                    shots = shots.assign(season = temp[i][\"season\"][\"season_name\"])\n",
    "    # Keep only the columns we need: Team name, Locations of shots, The shot outcome and the season of the match.\n",
    "    for i, shot in shots.iterrows():\n",
    "        shots_df.at[i,\"Team\"] = shot[\"team_name\"]\n",
    "        shots_df.at[i,\"Player\"] = shot[\"player_name\"]\n",
    "        shots_df.at[i,\"Body part\"] = shot[\"shot_body_part_name\"]\n",
    "        shots_df.at[i,\"Shot type\"] = shot[\"shot_type_name\"]\n",
    "        shots_df.at[i,\"Location\"] = shot[\"location\"]\n",
    "        shots_df.at[i,\"Shot_Outcome\"] = shot[\"shot_outcome_name\"]\n",
    "        shots_df.at[i,\"statsbomb_xg\"] = shot[\"shot_statsbomb_xg\"]\n",
    "        shots_df.at[i,\"Season\"] = shot[\"season\"]\n",
    "\n",
    "# Save the dataframe to a Csv file to be used in the application\n",
    "shots_df.to_csv(\"Real Betis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The resulted dataframe has the locations of all shots of Real Betis grouped by season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Number of matches of each team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the number of matches of each team we have to loop through all the files in matches repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary\n",
    "teams = defaultdict(lambda:0)\n",
    "\n",
    "# Loop through matches\n",
    "for competi in competitions_id:\n",
    "    files = os.listdir(matches + str(competi) )\n",
    "    for file in files:\n",
    "        with open(matches+ str(competi) + \"/\" + file) as f:\n",
    "            temp = json.load(f)\n",
    "            for i in range(len(temp)):\n",
    "                teams[temp[i]['home_team']['home_team_name']] +=1\n",
    "                teams[temp[i]['away_team']['away_team_name']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Goalscorers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the goalscorers of each team, we have to loop through events repository and amtches repository to group by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-d35501522496>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_m\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompeti\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"match_id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[0mgoals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgoals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"season\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"season_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "goalers = pd.DataFrame(columns=['Team', 'Player name', 'Season'])\n",
    "p = 'C:\\\\Users\\\\OMAR\\\\Desktop\\\\PFE\\\\Interface\\\\events\\\\'\n",
    "p_m = 'C:\\\\Users\\\\OMAR\\\\Desktop\\\\PFE\\\\Interface\\\\matches\\\\'\n",
    "\n",
    "path = os.listdir(p)\n",
    "path_m = os.listdir(p_m)\n",
    "competitions_id = [2,11,16,37,43,49,72]\n",
    "for file_name in path:\n",
    "    with open(p + file_name) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    df_t = pd.json_normalize(data, sep = \"_\").assign(match_id = file_name[:-5])\n",
    "    mid = df_t[\"match_id\"][0]   \n",
    "    goals = df_t.loc[df_t[\"shot_outcome_name\"] == \"Goal\"].set_index(\"id\")\n",
    "    goals = goals.loc[goals[\"team_name\"] == \"Real Betis\"]\n",
    "    for competi in competitions_id:\n",
    "        files = os.listdir(p_m + str(competi) )\n",
    "        for file in files:\n",
    "            with open(p_m + str(competi) + \"/\" + file) as f:\n",
    "                temp = json.load(f)\n",
    "            for i in range(len(temp)):\n",
    "                if temp[i][\"match_id\"] == pd.to_numeric(mid):\n",
    "                    goals = goals.assign(season = temp[i][\"season\"][\"season_name\"])\n",
    "    for i, goal in goals.iterrows():\n",
    "        goalers.at[i,\"Team\"] = goal[\"team_name\"]\n",
    "        goalers.at[i,\"Player name\"] = goal[\"player_name\"]\n",
    "        goalers.at[i,\"Season\"] = goal[\"season\"]\n",
    "goalers.to_csv(\"Barcelona.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: The teams and competition of each match id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('C:\\\\Users\\\\OMAR\\\\Desktop\\\\PFE\\\\Interface\\\\events\\\\')]\n",
    "for f in files:\n",
    "    with open('C:\\\\Users\\\\OMAR\\\\Desktop\\\\PFE\\\\Interface\\\\events\\\\'+f) as data_file:\n",
    "        mid = f[:-5]\n",
    "    d = json.load(data_file)\n",
    "    d = json_normalize(d, sep = \"_\").assign(mid = f[:-5])\n",
    "    print(\"Match id:\" + mid + \",\" + d[\"team_name\"][0] + \" Vs \" + d[\"team_name\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Test classifiers to create expected goals model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a data set to use\n",
    "shots_df = pd.read_csv(\"Barcelona.csv\")\n",
    "\n",
    "# Data preprocessing\n",
    "shots_df.Location = shots_df.Location.apply(lambda s: list(ast.literal_eval(s)))\n",
    "for i,shot in shots_df.iterrows():\n",
    "    shots_df.at[i,\"X\"] = shot[\"Location\"][0]\n",
    "    shots_df.at[i,\"Y\"] = shot[\"Location\"][1]\n",
    "    \n",
    "for i,shot in shots_df.iterrows():\n",
    "    if shot[\"Shot_Outcome\"] == \"Goal\":\n",
    "        shots_df.at[i,\"Goal\"] = 1\n",
    "    else:\n",
    "        shots_df.at[i,\"Goal\"] = 0\n",
    "        \n",
    "# Create Distance and Angle columns (Explained in the project report)\n",
    "    shots_df.at[i,'X'] = 120-shot[\"X\"]\n",
    "    x = shots_df.at[i,'X'] \n",
    "    y = abs(shots_df.at[i,'Y'] - 40)\n",
    "    \n",
    "    shots_df.at[i,'Distance'] = np.sqrt(x**2 + y**2)\n",
    "\n",
    "    a = np.arctan(8 * x /(x**2 + y**2 - (8/2)**2))\n",
    "    if a<0:\n",
    "        a=np.pi+a\n",
    "    shots_df.at[i,'Angle'] =a\n",
    "\n",
    "#Two dimensional histogram\n",
    "H_Shot=np.histogram2d(shots_df['X'], shots_df['Y'],bins=50,range=[[0, 120],[0, 80]])\n",
    "goals_only=shots_df[shots_df['Goal']==1]\n",
    "H_Goal=np.histogram2d(goals_only['X'], goals_only['Y'],bins=50,range=[[0, 120],[0, 80]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test three different models: Linear SVM, Logistic Regression, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the classifiers we would test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shots_df[[\"Distance\", \"Angle\", \"Body part\", \"Shot type\", 'Player']]\n",
    "y = shots_df[\"Goal\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then test the cross validation score of these classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFold_Score = pd.DataFrame()\n",
    "classifiers = ['Linear SVM', 'LogisticRegression','GradientBoostingClassifier']\n",
    "models = [svm.SVC(kernel='linear'),\n",
    "          LogisticRegression(max_iter = 1000),\n",
    "          GradientBoostingClassifier(random_state=0)\n",
    "         ]\n",
    "j = 0\n",
    "for i in models:\n",
    "    model = i\n",
    "    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    KFold_Score[classifiers[j]] = (cross_val_score(model, X, np.ravel(y), scoring = 'accuracy', cv=cv))\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = pd.DataFrame(KFold_Score.mean(), index= classifiers)\n",
    "KFold_Score = pd.concat([KFold_Score,mean.T])\n",
    "KFold_Score.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\n",
    "KFold_Score.T.sort_values(by=['Mean'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that GradientBoostingClassifier has the best average cross validation score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Training and tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use GridSearchCV to tune parameters of the model, we want to choose number of estimators, we set the interval of variables to test as [0,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "CV_gbc = GridSearchCV(estimator=model, param_grid={'n_estimators':range(0,20,1)}, scoring='roc_auc',n_jobs=4,iid=False, cv= 5)\n",
    "CV_gbc.fit(X_train,y_train)\n",
    "CV_gbc.best_params_, CV_gbc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of estimators we got is 12, now we move to maximum depth of the tree and minimum samples split parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {'max_depth':range(0,9,1), 'min_samples_split':range(100,600,100)}\n",
    "gbc = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.2, n_estimators=7, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gbc.fit(X_train,y_train)\n",
    "gbc.best_params_, gbc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep testing all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {'min_samples_split':range(100,1400,100), 'min_samples_leaf':range(0,20,2)}\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.2, n_estimators=7,max_depth=6, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch3.fit(X_train,y_train)\n",
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {'max_features':range(0,20,2)}\n",
    "gsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.2, n_estimators=7,max_depth=6, min_samples_split=800, min_samples_leaf=2, subsample=0.8, random_state=10),\n",
    "param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch4.fit(X_train,y_train)\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function returns the accuracy score, roc auc score, cv score and an histogram that shows us the importance of each feature(Angle and distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, pred, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], pred)\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_validate(alg, dtrain[predictors], pred, cv=cv_folds, scoring='roc_auc')\n",
    "    \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy :\",metrics.accuracy_score(pred.values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train):\", metrics.roc_auc_score(pred, dtrain_predprob))\n",
    "    print(\"cv Score: \", np.mean(cv_score['test_score']))\n",
    "        \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(gsearch3.best_estimator_, X_train, y_train, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(gsearch4.best_estimator_, X_train, y_train, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9,1]}\n",
    "gsearch5 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.2, n_estimators=7,max_depth=6,min_samples_split=800, min_samples_leaf=2, subsample=0.8, random_state=10,max_features=2),\n",
    "param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch5.fit(X_train,y_train)\n",
    "gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(gsearch5.best_estimator_, X_train, y_train, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_tuned_1 = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100,max_depth=6, min_samples_split=800,min_samples_leaf=2, subsample=0.8, random_state=10, max_features=2)\n",
    "modelfit(gbm_tuned_1, X_train, y_train, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_tuned_2 = GradientBoostingClassifier(learning_rate=0.05, n_estimators=1000,max_depth=6, min_samples_split=800,min_samples_leaf=2, subsample=0.8, random_state=10, max_features=2)\n",
    "modelfit(gbm_tuned_2, X_train, y_train, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_tuned_3 = GradientBoostingClassifier(learning_rate=0.001, n_estimators=1200,max_depth=6, min_samples_split=800,min_samples_leaf=2, subsample=0.8, random_state=10, max_features=2)\n",
    "modelfit(gbm_tuned_2, X_train, y_train, X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of gbm_tuned_3 has decreased, so we will fit gbm_tuned_2 to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gbm_tuned_2\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate probability(expected goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict_proba(shots_df[[\"Distance\", \"Angle\", \"Shot type\", \"Body part\", \"Player\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a scatter plot of distance and angle of shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax = sns.scatterplot(x=\"Distance\", y=\"Angle\", size = proba[:,1], hue=proba[:,1], data=shots_df)\n",
    "fig.set_size_inches(10,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the probability of a shot being a goal on a data sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_df = shots_df.assign(xg = proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_df[['Shot_Outcome', 'statsbomb_xg', 'xg']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model as a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"model\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Omar El Yousfi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
